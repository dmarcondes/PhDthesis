}
#If exhausted more points than desired
if(!optimal){
if(vis >= exhaust)
break
}
if(verbose)
cat("*") #SLM symbol
}
}
}
if(optimal){
remain <- length(colnames(Lh)) #Number of nodes remaining
#If nodes remaining start exhaustion
if(remain > 0 & remain <= stop){
finished <- 1
if(verbose){
cat("\n")
cat(paste("Ending U-curve algorithm and starting exhaustion of ",ncol(Lh)," (",
ifelse(n <= 218,round(100*ncol(Lh)/bell(n),5),0),
"%) nodes remaining...",sep = ""))
cat("\n")
}
#Get error of all remaining
err <- unlist(mclapply(data.frame(rbind(colnames(Lh))),function(x) getError(getPartition(x),jtrain,jval,increasing,n),mc.cores = cores))
#Get minimum error of strong local minimums
m <- min(unlist(mclapply(data.frame(rbind(strongMinimums)),function(x) getError(getPartition(x),jtrain,jval,increasing,n),mc.cores = cores)))
#Add nodes with less error to strong local minimums
strongMinimums <- c(strongMinimums,colnames(Lh)[err <= m])
if(verbose){
cat("\n")
cat("---------------------------------------------------------------------------------")
cat("\n")
cat(paste("There was ",remain," (",ifelse(n <= 218,round(100*remain/bell(n),5),0),"%)"," nodes to visit after exhausting ",
vis," (",ifelse(n <= 218,round(100*vis/bell(n),5),0),"%)",sep = ""))
cat("\n")
cat("---------------------------------------------------------------------------------")
}
}
else if(remain == 0){
finished <- 1
if(verbose){
cat("\n")
cat("---------------------------------------------------------------------------------")
cat("\n")
cat(paste("There was ",remain," (",ifelse(n <= 218,round(100*remain/bell(n),5),0),"%)"," nodes to visit after exhausting ",
vis," (",ifelse(n <= 218,round(100*vis/bell(n),5),0),"%)",sep = ""))
cat("\n")
cat("---------------------------------------------------------------------------------")
}
}
else{
finished <- 0
if(verbose){
cat("\n")
cat("---------------------------------------------------------------------------------")
cat("\n")
cat(paste("There was ",remain," (",ifelse(n <= 218,round(100*remain/bell(n),5),0),"%)"," nodes not visited early stopping afeter exhausting ",
vis," (",ifelse(n <= 218,round(100*vis/bell(n),5),0),"%)",sep = ""))
cat("\n")
cat("---------------------------------------------------------------------------------")
}
}
}
#Solution
e <- apply(rbind(strongMinimums),2,function(x) getError(getPartition(x),jtrain,jval,increasing,n)) #Error of SLM
global <- strongMinimums[e == min(e)] #Minimums of SLMs
global <- apply(rbind(global),2,function(x) getPartition(x)) #Get partition of minimums
solution <- unique(unlist(lapply(global,function(x) namePartition(makePartition(x))))) #Get name of minimums partition
#Solution
oh <- tapply(cbind(solution),solution,function(x) optimalHyp(part = x,jtrain = jtrain,increasing = increasing,n)) #Get hypotheses estimated of solutions
names(oh) <- NULL
if(!increasing)
oh <- lapply(oh,function(x) data.frame("X" = X,"fx" = plyr::mapvalues(factor(x[,2]),c("0","1"),Y)))
error <- unique(tapply(cbind(solution),solution,function(x) getError(part = getPartition(x),jtrain = jtrain,jval = jval,increasing,n))) #Error of solution
names(error) <- NULL
#Timing
end_time <- Sys.time()
#Classifier
classifier <- function(x){
y <- unlist(lapply(unique(oh),function(y) y[y[,1] == x,2]))
y <- table(y)
y <- names(y)[y == max(y)]
if(length(y) > 1)
y <- sample(y,1)
return(y)
}
list("classifier" = classifier,"hypotheses" = unique(oh),"partitions" = solution,"error" = error,
"exhausted" = vis,"remain" = ifelse(optimal,remain,NA),"finished" = finished,"SLMvis" = ifelse(!finished,SLMvis,NA),
"remain_after_prune" = remain_after_prune,"exhausted_until_prune" = exhausted_until_prune,
"time" = difftime(end_time,start_time,units = "mins"),"optimal" = optimal,"plot" = Plot)
#Read lattice
Lh <- fst::read.fst(paste("part_",m,".fst",sep = ""),)
m
m=8
#Read lattice
Lh <- fst::read.fst(paste("part_",m,".fst",sep = ""))
#Tab to store solutions
tab <- data.frame("Optimal" = NA,"n" = NA,"exhausted" = NA,"remain" = NA,"number solutions" = NA,"val error" = NA,"real error" = NA,"ERM error" = NA,"Mhat error" = NA,
"time" = NA)
trace <- NULL
info <- NULL
#Joint as tab
tab_joint <- data.table::rbindlist(joint) %>% data.frame()
tab_joint$prob <- prob
tab_joint <- tab_joint %>% spread("Var1","prob")
tab_joint <- cbind(tab_joint[,2],tab_joint[,3])
n <- 64
reps <- 10
#For each sample size
for(size in n){
cat("\n")
cat(paste("Sample size =",size))
cat("\n")
#Train, validation and independent sample size
ntrain <- floor(0.5*size)
nval <- floor(0.25*size)
nindependent <- size - ntrain - nval
#Vectors to store results optimal search
nsolutions <- vector()
val_error <- vector()
real_error <- vector()
erm_error <- vector()
Mhat_error <- vector()
remain <- vector()
exhausted <- vector()
time <- vector()
#Vectors to store results suboptimal search
est_nsolutions <- vector()
est_val_error <- vector()
est_real_error <- vector()
est_erm_error <- vector()
est_Mhat_error <- vector()
est_remain <- vector()
est_exhausted <- vector()
est_time <- vector()
#Repeat
for(i in 1:rep){
cat("\n")
cat(i)
cat("\n")
#Sample train data
train <- data.frame(data.table::rbindlist(sample(x = joint,size = ntrain,replace = T,prob = prob)))
names(train) <- c("y","x")
#Sample validation data
val <- data.frame(data.table::rbindlist(sample(x = joint,size = nval,replace = T,prob = prob)))
names(val) <- c("y","x")
#Sample independent data
independent <- data.frame(data.table::rbindlist(sample(x = joint,size = nindependent,replace = T,prob = prob)))
names(independent) <- c("y","x")
#Domains
independent$x <- factor(independent$x,Xd)
independent$y <- factor(independent$y,Yd)
#Run U-curve Optimal
u <- ucurve(xtrain = train$x,ytrain = train$y,xval = val$x,yval = val$y,Lh = Lh,cores = cores,stop = stop,verbose = verbose,X = Xd,
Y = Yd)
#Get estimated hypotheses of each returned partition
est_h <- unique(lapply(rbind(u$partitions),function(x) optimalHyp(part = x,jtrain = prop.table(table(independent$x,independent$y)),
increasing = F,n = length(unique(independent$x)))))
#Get target hypotheses of returned partitions
hstar <- unique(lapply(rbind(u$partitions),function(x) optimalHyp(part = x,jtrain = tab_joint,
increasing = F,n = length(unique(independent$x)))))
#Get ERM hypotheses of learning directly in H
erm_hyp <- optimalHyp(part = namePartition(makePartition(as.list(1:m))),
jtrain = prop.table(table(c(train$x,val$x,independent$x),c(train$y,val$y,as.character(independent$y)))),increasing = F,
n = length(unique(c(train$x,val$x,independent$x))))
#Store results
exhausted[i] <- u$exhausted #Number of models exhausted
remain[i] <- u$remain #Models remaining when
nsolutions[i] <- length(unique(est_h)) #Number of solutions
val_error[i] <- u$error #Validation error
real_error[i] <- min(unlist(lapply(est_h,function(x) realError(x,joint,prob)))) #Real error of best estimated hypotheses
Mhat_error[i] <- min(unlist(lapply(hstar,function(x) realError(x,joint,prob)))) #Real error of Mhat
time[i] <- u$time #Running time
erm_error[i] <- realError(erm_hyp,joint,prob) #Real error of best erm hypotheses
#Store trace of algorithm
if(is.null(trace))
trace <- data.frame("optimal" = "Yes","n" = size,"id" = i,"ex" = u$exhausted_until_prune,"rem" = u$remain_after_prune)
else
trace <- rbind.data.frame(trace,data.frame("optimal" = "Yes","n" = size,"id" = i,"ex" = u$exhausted_until_prune,"rem" = u$remain_after_prune))
#Run Suboptimal U-curve un
u <- ucurve(xtrain = train$x,ytrain = train$y,xval = val$x,yval = val$y,Lh = Lh,cores = cores,stop = stop,verbose = verbose,X = Xd,
Y = Yd,optimal = F,exhaust = exhaust)
#Get estimated hypotheses of each returned partition
est_h <- unique(lapply(rbind(u$partitions),function(x) optimalHyp(part = x,jtrain = prop.table(table(independent$x,independent$y)),
increasing = F,n = length(unique(independent$x)))))
#Get target hypotheses of returned partitions
hstar <- unique(lapply(rbind(u$partitions),function(x) optimalHyp(part = x,jtrain = tab_joint,
increasing = F,n = length(unique(independent$x)))))
#Get ERM hypotheses of learning directly in H
erm_hyp <- optimalHyp(part = namePartition(makePartition(as.list(1:m))),
jtrain = prop.table(table(c(train$x,val$x,independent$x),c(train$y,val$y,as.character(independent$y)))),increasing = F,
n = length(unique(c(train$x,val$x,independent$x))))
#Store results
est_exhausted[i] <- u$exhausted #Number of models exhausted
est_remain[i] <- u$remain #Models remaining when
est_nsolutions[i] <- length(unique(est_h)) #Number of solutions
est_val_error[i] <- u$error #Validation error
est_real_error[i] <- min(unlist(lapply(est_h,function(x) realError(x,joint,prob)))) #Real error of best estimated hypotheses
est_Mhat_error[i] <- min(unlist(lapply(hstar,function(x) realError(x,joint,prob)))) #Real error of Mhat
est_time[i] <- u$time #Running time
est_erm_error[i] <- realError(erm_hyp,joint,prob) #Real error of best erm hypotheses
#Store trace of algorithm
trace <- rbind.data.frame(trace,data.frame("optimal" = "No","n" = size,"id" = i,"ex" = u$exhausted_until_prune,"rem" = u$remain_after_prune))
rm(train,val,independent,u,est_h)
}
#Summarize optimal search with given sample size
tab <- rbind.data.frame(tab,data.frame("Optimal" = "Yes","n" = size,
"exhausted" = paste(round(median(exhausted),2)," (",
round(quantile(exhausted,0.025),2),",",
round(quantile(exhausted,0.975),2),")",sep = ""),
"remain" = paste(round(median(remain),2)," (",
round(quantile(remain,0.025),2),",",
round(quantile(remain,0.975),2),")",sep = ""),
"number solutions" = paste(median(nsolutions)," (",
quantile(nsolutions,0.025),",",
quantile(nsolutions,0.975),")",sep = ""),
"val error" = paste(round(median(val_error),5)," (",
round(quantile(val_error,0.025),5),",",
round(quantile(val_error,0.975),5),")",sep = ""),
"real error" = paste(round(median(real_error),5)," (",
round(quantile(real_error,0.025),5),",",
round(quantile(real_error,0.975),5),")",sep = ""),
"ERM error" = paste(round(median(erm_error),5)," (",
round(quantile(erm_error,0.025),5),",",
round(quantile(erm_error,0.975),5),")",sep = ""),
"Mhat error" = paste(round(median(Mhat_error),5)," (",
round(quantile(Mhat_error,0.025),5),",",
round(quantile(Mhat_error,0.975),5),")",sep = ""),
"time" = paste(round(median(time),5)," (",
round(quantile(time,0.025),5),",",
round(quantile(time,0.975),5),")",sep = "")))
#Summarize suboptimal search with given sample size
tab <- rbind.data.frame(tab,data.frame("Optimal" = "No","n" = size,
"exhausted" = paste(round(median(est_exhausted),2)," (",
round(quantile(est_exhausted,0.025),2),",",
round(quantile(est_exhausted,0.975),2),")",sep = ""),
"remain" = NA,
"number solutions" = paste(median(est_nsolutions)," (",
quantile(est_nsolutions,0.025),",",
quantile(est_nsolutions,0.975),")",sep = ""),
"val error" = paste(round(median(est_val_error),5)," (",
round(quantile(est_val_error,0.025),5),",",
round(quantile(est_val_error,0.975),5),")",sep = ""),
"real error" = paste(round(median(est_real_error),5)," (",
round(quantile(est_real_error,0.025),5),",",
round(quantile(est_real_error,0.975),5),")",sep = ""),
"ERM error" = paste(round(median(est_erm_error),5)," (",
round(quantile(est_erm_error,0.025),5),",",
round(quantile(est_erm_error,0.975),5),")",sep = ""),
"Mhat error" = paste(round(median(est_Mhat_error),5)," (",
round(quantile(est_Mhat_error,0.025),5),",",
round(quantile(est_Mhat_error,0.975),5),")",sep = ""),
"time" = paste(round(median(est_time),5)," (",
round(quantile(est_time,0.025),5),",",
round(quantile(est_time,0.975),5),")",sep = "")))
#Store all information about simulations
if(is.null(info))
info <- data.frame("id"= 1:rep,"Optimal" = "Yes","n" = size,"exhausted" = exhausted,"remain" = remain,"number solutions" = nsolutions,
"val error" = val_error,"real error" = real_error,"ERM error" = erm_error,
"Mhat error" = Mhat_error,
"time" = time)
else
info <- rbind.data.frame(info,data.frame("id"= 1:rep,"Optimal" = "Yes","n" = size,"exhausted" = exhausted,"remain" = remain,"number solutions" = nsolutions,
"val error" = val_error,"real error" = real_error,"ERM error" = erm_error,
"Mhat error" = Mhat_error,
"time" = time))
info <- rbind.data.frame(info,data.frame("id"= 1:rep,"Optimal" = "No","n" = size,"exhausted" = est_exhausted,"remain" = est_remain,"number solutions" = est_nsolutions,
"val error" = est_val_error,"real error" = est_real_error,"ERM error" = est_erm_error,
"Mhat error" = est_Mhat_error,
"time" = est_time))
#Save information stored so far
write.table(x = na.omit(tab),file = paste(path,".txt",sep = ""))
write_rds(x = list("table" = na.omit(tab),"trace" = trace,"info" = info),file = paste(path,".rds",sep = ""))
}
exhaust = 100
#Read lattice
Lh <- fst::read.fst(paste("part_",m,".fst",sep = ""))
#Tab to store solutions
tab <- data.frame("Optimal" = NA,"n" = NA,"exhausted" = NA,"remain" = NA,"number solutions" = NA,"val error" = NA,"real error" = NA,"ERM error" = NA,"Mhat error" = NA,
"time" = NA)
trace <- NULL
info <- NULL
#Joint as tab
tab_joint <- data.table::rbindlist(joint) %>% data.frame()
tab_joint$prob <- prob
tab_joint <- tab_joint %>% spread("Var1","prob")
tab_joint <- cbind(tab_joint[,2],tab_joint[,3])
#For each sample size
for(size in n){
cat("\n")
cat(paste("Sample size =",size))
cat("\n")
#Train, validation and independent sample size
ntrain <- floor(0.5*size)
nval <- floor(0.25*size)
nindependent <- size - ntrain - nval
#Vectors to store results optimal search
nsolutions <- vector()
val_error <- vector()
real_error <- vector()
erm_error <- vector()
Mhat_error <- vector()
remain <- vector()
exhausted <- vector()
time <- vector()
#Vectors to store results suboptimal search
est_nsolutions <- vector()
est_val_error <- vector()
est_real_error <- vector()
est_erm_error <- vector()
est_Mhat_error <- vector()
est_remain <- vector()
est_exhausted <- vector()
est_time <- vector()
#Repeat
for(i in 1:rep){
cat("\n")
cat(i)
cat("\n")
#Sample train data
train <- data.frame(data.table::rbindlist(sample(x = joint,size = ntrain,replace = T,prob = prob)))
names(train) <- c("y","x")
#Sample validation data
val <- data.frame(data.table::rbindlist(sample(x = joint,size = nval,replace = T,prob = prob)))
names(val) <- c("y","x")
#Sample independent data
independent <- data.frame(data.table::rbindlist(sample(x = joint,size = nindependent,replace = T,prob = prob)))
names(independent) <- c("y","x")
#Domains
independent$x <- factor(independent$x,Xd)
independent$y <- factor(independent$y,Yd)
#Run U-curve Optimal
u <- ucurve(xtrain = train$x,ytrain = train$y,xval = val$x,yval = val$y,Lh = Lh,cores = cores,stop = stop,verbose = verbose,X = Xd,
Y = Yd)
#Get estimated hypotheses of each returned partition
est_h <- unique(lapply(rbind(u$partitions),function(x) optimalHyp(part = x,jtrain = prop.table(table(independent$x,independent$y)),
increasing = F,n = length(unique(independent$x)))))
#Get target hypotheses of returned partitions
hstar <- unique(lapply(rbind(u$partitions),function(x) optimalHyp(part = x,jtrain = tab_joint,
increasing = F,n = length(unique(independent$x)))))
#Get ERM hypotheses of learning directly in H
erm_hyp <- optimalHyp(part = namePartition(makePartition(as.list(1:m))),
jtrain = prop.table(table(c(train$x,val$x,independent$x),c(train$y,val$y,as.character(independent$y)))),increasing = F,
n = length(unique(c(train$x,val$x,independent$x))))
#Store results
exhausted[i] <- u$exhausted #Number of models exhausted
remain[i] <- u$remain #Models remaining when
nsolutions[i] <- length(unique(est_h)) #Number of solutions
val_error[i] <- u$error #Validation error
real_error[i] <- min(unlist(lapply(est_h,function(x) realError(x,joint,prob)))) #Real error of best estimated hypotheses
Mhat_error[i] <- min(unlist(lapply(hstar,function(x) realError(x,joint,prob)))) #Real error of Mhat
time[i] <- u$time #Running time
erm_error[i] <- realError(erm_hyp,joint,prob) #Real error of best erm hypotheses
#Store trace of algorithm
if(is.null(trace))
trace <- data.frame("optimal" = "Yes","n" = size,"id" = i,"ex" = u$exhausted_until_prune,"rem" = u$remain_after_prune)
else
trace <- rbind.data.frame(trace,data.frame("optimal" = "Yes","n" = size,"id" = i,"ex" = u$exhausted_until_prune,"rem" = u$remain_after_prune))
#Run Suboptimal U-curve un
u <- ucurve(xtrain = train$x,ytrain = train$y,xval = val$x,yval = val$y,Lh = Lh,cores = cores,stop = stop,verbose = verbose,X = Xd,
Y = Yd,optimal = F,exhaust = exhaust)
#Get estimated hypotheses of each returned partition
est_h <- unique(lapply(rbind(u$partitions),function(x) optimalHyp(part = x,jtrain = prop.table(table(independent$x,independent$y)),
increasing = F,n = length(unique(independent$x)))))
#Get target hypotheses of returned partitions
hstar <- unique(lapply(rbind(u$partitions),function(x) optimalHyp(part = x,jtrain = tab_joint,
increasing = F,n = length(unique(independent$x)))))
#Get ERM hypotheses of learning directly in H
erm_hyp <- optimalHyp(part = namePartition(makePartition(as.list(1:m))),
jtrain = prop.table(table(c(train$x,val$x,independent$x),c(train$y,val$y,as.character(independent$y)))),increasing = F,
n = length(unique(c(train$x,val$x,independent$x))))
#Store results
est_exhausted[i] <- u$exhausted #Number of models exhausted
est_remain[i] <- u$remain #Models remaining when
est_nsolutions[i] <- length(unique(est_h)) #Number of solutions
est_val_error[i] <- u$error #Validation error
est_real_error[i] <- min(unlist(lapply(est_h,function(x) realError(x,joint,prob)))) #Real error of best estimated hypotheses
est_Mhat_error[i] <- min(unlist(lapply(hstar,function(x) realError(x,joint,prob)))) #Real error of Mhat
est_time[i] <- u$time #Running time
est_erm_error[i] <- realError(erm_hyp,joint,prob) #Real error of best erm hypotheses
#Store trace of algorithm
trace <- rbind.data.frame(trace,data.frame("optimal" = "No","n" = size,"id" = i,"ex" = u$exhausted_until_prune,"rem" = u$remain_after_prune))
rm(train,val,independent,u,est_h)
}
#Summarize optimal search with given sample size
tab <- rbind.data.frame(tab,data.frame("Optimal" = "Yes","n" = size,
"exhausted" = paste(round(median(exhausted),2)," (",
round(quantile(exhausted,0.025),2),",",
round(quantile(exhausted,0.975),2),")",sep = ""),
"remain" = paste(round(median(remain),2)," (",
round(quantile(remain,0.025),2),",",
round(quantile(remain,0.975),2),")",sep = ""),
"number solutions" = paste(median(nsolutions)," (",
quantile(nsolutions,0.025),",",
quantile(nsolutions,0.975),")",sep = ""),
"val error" = paste(round(median(val_error),5)," (",
round(quantile(val_error,0.025),5),",",
round(quantile(val_error,0.975),5),")",sep = ""),
"real error" = paste(round(median(real_error),5)," (",
round(quantile(real_error,0.025),5),",",
round(quantile(real_error,0.975),5),")",sep = ""),
"ERM error" = paste(round(median(erm_error),5)," (",
round(quantile(erm_error,0.025),5),",",
round(quantile(erm_error,0.975),5),")",sep = ""),
"Mhat error" = paste(round(median(Mhat_error),5)," (",
round(quantile(Mhat_error,0.025),5),",",
round(quantile(Mhat_error,0.975),5),")",sep = ""),
"time" = paste(round(median(time),5)," (",
round(quantile(time,0.025),5),",",
round(quantile(time,0.975),5),")",sep = "")))
#Summarize suboptimal search with given sample size
tab <- rbind.data.frame(tab,data.frame("Optimal" = "No","n" = size,
"exhausted" = paste(round(median(est_exhausted),2)," (",
round(quantile(est_exhausted,0.025),2),",",
round(quantile(est_exhausted,0.975),2),")",sep = ""),
"remain" = NA,
"number solutions" = paste(median(est_nsolutions)," (",
quantile(est_nsolutions,0.025),",",
quantile(est_nsolutions,0.975),")",sep = ""),
"val error" = paste(round(median(est_val_error),5)," (",
round(quantile(est_val_error,0.025),5),",",
round(quantile(est_val_error,0.975),5),")",sep = ""),
"real error" = paste(round(median(est_real_error),5)," (",
round(quantile(est_real_error,0.025),5),",",
round(quantile(est_real_error,0.975),5),")",sep = ""),
"ERM error" = paste(round(median(est_erm_error),5)," (",
round(quantile(est_erm_error,0.025),5),",",
round(quantile(est_erm_error,0.975),5),")",sep = ""),
"Mhat error" = paste(round(median(est_Mhat_error),5)," (",
round(quantile(est_Mhat_error,0.025),5),",",
round(quantile(est_Mhat_error,0.975),5),")",sep = ""),
"time" = paste(round(median(est_time),5)," (",
round(quantile(est_time,0.025),5),",",
round(quantile(est_time,0.975),5),")",sep = "")))
#Store all information about simulations
if(is.null(info))
info <- data.frame("id"= 1:rep,"Optimal" = "Yes","n" = size,"exhausted" = exhausted,"remain" = remain,"number solutions" = nsolutions,
"val error" = val_error,"real error" = real_error,"ERM error" = erm_error,
"Mhat error" = Mhat_error,
"time" = time)
else
info <- rbind.data.frame(info,data.frame("id"= 1:rep,"Optimal" = "Yes","n" = size,"exhausted" = exhausted,"remain" = remain,"number solutions" = nsolutions,
"val error" = val_error,"real error" = real_error,"ERM error" = erm_error,
"Mhat error" = Mhat_error,
"time" = time))
info <- rbind.data.frame(info,data.frame("id"= 1:rep,"Optimal" = "No","n" = size,"exhausted" = est_exhausted,"remain" = est_remain,"number solutions" = est_nsolutions,
"val error" = est_val_error,"real error" = est_real_error,"ERM error" = est_erm_error,
"Mhat error" = est_Mhat_error,
"time" = est_time))
#Save information stored so far
write.table(x = na.omit(tab),file = paste(path,".txt",sep = ""))
write_rds(x = list("table" = na.omit(tab),"trace" = trace,"info" = info),file = paste(path,".rds",sep = ""))
}
info
#Summarize optimal search with given sample size
tab <- rbind.data.frame(tab,data.frame("Optimal" = "Yes","n" = size,
"exhausted" = paste(round(median(exhausted),2)," (",
round(quantile(exhausted,0.025),2),",",
round(quantile(exhausted,0.975),2),")",sep = ""),
"remain" = paste(round(median(remain),2)," (",
round(quantile(remain,0.025),2),",",
round(quantile(remain,0.975),2),")",sep = ""),
"number solutions" = paste(median(nsolutions)," (",
quantile(nsolutions,0.025),",",
quantile(nsolutions,0.975),")",sep = ""),
"val error" = paste(round(median(val_error),5)," (",
round(quantile(val_error,0.025),5),",",
round(quantile(val_error,0.975),5),")",sep = ""),
"real error" = paste(round(median(real_error),5)," (",
round(quantile(real_error,0.025),5),",",
round(quantile(real_error,0.975),5),")",sep = ""),
"ERM error" = paste(round(median(erm_error),5)," (",
round(quantile(erm_error,0.025),5),",",
round(quantile(erm_error,0.975),5),")",sep = ""),
"Mhat error" = paste(round(median(Mhat_error),5)," (",
round(quantile(Mhat_error,0.025),5),",",
round(quantile(Mhat_error,0.975),5),")",sep = ""),
"time" = paste(round(median(time),5)," (",
round(quantile(time,0.025),5),",",
round(quantile(time,0.975),5),")",sep = "")))
tab
#Summarize suboptimal search with given sample size
tab <- rbind.data.frame(tab,data.frame("Optimal" = "No","n" = size,
"exhausted" = paste(round(median(est_exhausted),2)," (",
round(quantile(est_exhausted,0.025),2),",",
round(quantile(est_exhausted,0.975),2),")",sep = ""),
"remain" = NA,
"number solutions" = paste(median(est_nsolutions)," (",
quantile(est_nsolutions,0.025),",",
quantile(est_nsolutions,0.975),")",sep = ""),
"val error" = paste(round(median(est_val_error),5)," (",
round(quantile(est_val_error,0.025),5),",",
round(quantile(est_val_error,0.975),5),")",sep = ""),
"real error" = paste(round(median(est_real_error),5)," (",
round(quantile(est_real_error,0.025),5),",",
round(quantile(est_real_error,0.975),5),")",sep = ""),
"ERM error" = paste(round(median(est_erm_error),5)," (",
round(quantile(est_erm_error,0.025),5),",",
round(quantile(est_erm_error,0.975),5),")",sep = ""),
"Mhat error" = paste(round(median(est_Mhat_error),5)," (",
round(quantile(est_Mhat_error,0.025),5),",",
round(quantile(est_Mhat_error,0.975),5),")",sep = ""),
"time" = paste(round(median(est_time),5)," (",
round(quantile(est_time,0.025),5),",",
round(quantile(est_time,0.975),5),")",sep = "")))
View(tab)
#Run Suboptimal U-curve un
u <- ucurve(xtrain = train$x,ytrain = train$y,xval = val$x,yval = val$y,Lh = Lh,cores = cores,stop = stop,verbose = verbose,X = Xd,
Y = Yd,optimal = F,exhaust = exhaust,sampleNeigh = 10)
u$time
install.packages("~/GDrive/Doutorado/CÃ³digos/partitionUcurve_0.1.tar.gz", repos = NULL, type = "source")
